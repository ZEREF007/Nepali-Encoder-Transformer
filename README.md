# Nepali-Encoder-Transformers

**Nepali Encoder Transformers: An Analysis of Auto Encoding Transformer Language Models for Nepali Text Classification**  
European Language Resources Association (ELRA) · Jun 25, 2022

Our paper investigates effective methods for pretraining a Transformer-based model specifically for the Nepali language. We address the language-specific aspects critical for modeling. While some models have been trained for Nepali, the existing research is insufficient. We train and evaluate three Transformer-based masked language models for Nepali text sequences:

- **distilbert-base** for its efficiency and compactness
- **deberta-base** for its ability to model token dependencies effectively
- **XLM-ROBERTa** for its proficiency in handling multilingual tasks

For more details, refer to the paper: [Link to Paper](http://www.lrec-conf.org/proceedings/lrec2022/workshops/SIGUL/pdf/2022.sigul-1.14.pdf)

• Tech Stack: Distilbert-base, Deberta-base, XLM-ROBERTa, Sentence Piece Model (SPM),
Transformers, PyTorch, Datasets, Tokenizers

